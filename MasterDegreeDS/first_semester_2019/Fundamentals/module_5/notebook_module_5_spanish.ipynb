{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Programación para *Data Science*\n",
    "============================\n",
    "\n",
    "Unidad 5: Adquisición de datos en Python\n",
    "--------------------------------------\n",
    "\n",
    "\n",
    "En este Notebook encontraréis dos conjuntos de ejercicios: un primer conjunto de **ejercicios para practicar** y un segundo conjunto de **actividades evaluables** como PEC de la asignatura.\n",
    "\n",
    "En cuanto al conjunto de ejercicios para practicar, éstos no puntúan para la PEC, pero os recomendamos que los intentéis resolver como parte del proceso de aprendizaje. Encontraréis ejemplos de posibles soluciones a los ejercicios al propio notebook, pero es importante que intentéis resolverlos vosotros antes de consultar las soluciones. Las soluciones os permitirán validar vuestras respuestas, así como ver alternativas de resolución de las actividades. También os animamos a preguntar cualquier duda que surja sobre la resolución de los **ejercicios para practicar** en el foro del aula.\n",
    "\n",
    "\n",
    "Además, veréis que todas las actividades tienen una etiqueta que indica los recursos necesarios para llevarla a cabo. Hay tres posibles etiquetas:\n",
    "\n",
    "* <span style=\"font-family: Courier New; background-color: #82b74b; color: #000000; padding: 3px; \">NM</span> **Sólo materiales**: las herramientas necesarias para realizar la actividad se pueden encontrar en los materiales de la asignatura. \n",
    "\n",
    "* <span style=\"font-family: Courier New; background-color: #ffcc5c; color: #000000; padding: 3px; \">EG</span> **Consulta externa guiada**: la actividad puede requerir hacer uso de herramientas que no se encuentran en los materiales de la asignatura, pero el enunciado contiene indicaciones de dónde o cómo encontrar la información adicional necesaria para resolver la actividad.\n",
    "\n",
    "* <span style=\"font-family: Courier New; background-color: #f2ae72; color: #000000; padding: 3px; \">EI</span> **Consulta externa independente**: la actividad puede requerir hacer uso de herramientas que no se encuentran en los materiales de la asignatura, y el enunciado puede no incluir la descripción de dónde o cómo encontrar esta información adicional. Será necesario que el estudiante busque esta información utilizando los recursos que se han explicado en la asignatura.\n",
    "\n",
    "Es importante notar que estas etiquetas no indican el nivel de dificultad del ejercicio, sino únicamente la necesidad de consulta de documentación externa para su resolución. Además, recordad que las **etiquetas son informativas**, pero podréis consultar referencias externas en cualquier momento (aunque no se indique explícitamente) o puede ser que podáis hacer una actividad sin consultar ningún tipo de documentación. Por ejemplo, para resolver una actividad que sólo requiera los materiales de la asignatura, puedéis consultar referencias externas si queréis, ya sea tanto para ayudaros en la resolución como para ampliar el conocimiento!\n",
    "\n",
    "En cuanto a la consulta de documentación externa en la resolución de los ejercicios, recordad **citar siempre la bibliografía utilizada** para resolver cada actividad."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejercicios para practicar\n",
    "\n",
    "**Los siguientes 3 ejercicios no puntúan para la PEC**, pero os recomendamos que los intentéis resolver antes de pasar a los ejercicios propios de la PEC. También encontraréis las soluciones a estos ejercicios al final del Notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejercicio 1\n",
    "Queremos saber los crímenes que se han producido en Reino Unido en una localización (latitud, longitud) y fecha concretas. Identificad qué métodos de la API siguiente podemos utilizar para obtener la información y contestad a las siguientes preguntas.<span style=\"font-family: Courier New; background-color: #82b74b; color: #000000; padding: 3px; \">NM</span> \n",
    "\n",
    "1. ¿A qué URL haremos la petición?\n",
    "2. ¿Qué tipo de petición HTTP (qué acción) deberemos realizar contra la API para obtener los datos deseados?\n",
    "3. ¿En qué formato obtendremos la respuesta de la API?\n",
    "4. ¿Qué parámetros deberemos proporcionar en la petición a la API?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Respuesta**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejercicio 2 \n",
    "Programad una función que retorne el estado meteorológico actual en una cierta localización, definida por su código postal (**zip code**) y código de país (e.g: us, uk, es, fr, etc). La función debe devolver una lista de tuplas de dos elementos, correspondientes al resumen del estado actual del tiempo **(weather.main)** y a la descripción extendida **(weather.description)**. Utilizad la API de [openweathermap](https://openweathermap.org/api) para obtener las predicciones.\n",
    "\n",
    "Para utilizar la API necesitareis registraros y obtener una API key. Podéis registraros [aquí](https://home.openweathermap.org/users/sign_up) y obtener vuestra API key [aquí](https://home.openweathermap.org/api_keys) una vez registrados. Tened en cuenta que la API key puede tardar un rato en funcionar después de registraros, y la API os devolverá un error 401 conforme la clave no es valida:\n",
    "\n",
    "`{\"cod\":401, \"message\": \"Invalid API key. Please see http://openweathermap.org/faq#error401 for more info.\"}`\n",
    "\n",
    "Simplemente esperad un rato antes de utilizar la clave.<span style=\"font-family: Courier New; background-color: #82b74b; color: #000000; padding: 3px; \">NM</span>\n",
    "\n",
    "**Hints**: \n",
    "\n",
    "- Veréis que en general la API esta documentada sin incluir la API key, aun que esta es necesaria. Deberéis incluir la API key en la llamada como uno de los parámetros de la URL (&appid=your_api_key):\n",
    "\n",
    "    http://example_url.com?param1=value1&param2=value2&appid=your_api_key\n",
    "    \n",
    "\n",
    "- Os animamos a que paséis por el proceso de registro para que veáis de que trata y cómo se generan las API keys. Aún así, os proporcionamos una API key en caso de que tengáis problemas con el proceso.\n",
    "\n",
    "    owm_api_key = 'd54f26dbcf6d4136bc0ef8ba5f07825b'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Respuesta**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejercicio 3\n",
    "[CoinMarketCap](https://coinmarketcap.com/) es una web con contenido acerca de\n",
    "las 100 criptomonedas con más capitalización de mercado. Programad un _crawler_ que extraiga los nombres y la capitalización de todas les monedas que se muestran en CoinMarketCap. Utilizad la estructura de _crawler_ que hemos visto en el Notebook de esta unidad **modificando únicamente dos lineas de código**:<span style=\"font-family: Courier New; background-color: #ffcc5c; color: #000000; padding: 3px; \">EG</span>\n",
    "- URL de inicio.\n",
    "- La expresión XPath que selecciona el contenido a capturar.\n",
    "\n",
    "**Pista**: tal vez os puede ser de utilidad investigar sobre la scrapy shell y utilizarla para encontrar la expresión XPath que necesitas para resolver el ejercicio.\n",
    "\n",
    "**Nota**: si la ejecución del _crawler_ os devuelve un error `ReactorNotRestartable`, reiniciad el núcleo del Notebook (en el meú: `Kernel` - `Restart`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Respuesta**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejercicios y preguntas teóricas para la PEC\n",
    "\n",
    "A continuación, encontraréis los **ejercicios y preguntas teóricas que debéis completar en esta PEC** y que forman parte de la evaluación de esta unidad."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejercicio 1\n",
    "\n",
    "\n",
    "La librería Tweepy nos permite interactuar con la API de Twitter de una forma sencilla.  Utilizando la librería Tweepy, recuperad la descripción, la fecha de creación y localización de vuestra cuenta de Twitter.  Si lo preferís, podéis obtener dicha información de la cuenta del usuario de Twitter de la Python Software Foundation, `ThePSF`( en vez de utilizar vuestra cuenta). **(1 punto)** <span style=\"font-family: Courier New; background-color: #82b74b; color: #000000; padding: 2px; \">NM</span>\n",
    "\n",
    "**Nota**: Necesitáis las claves ***Consumer API keys y Access token & acces token secret***.  Para obtener las claves, seguid las indicaciones que encontraréis en el Notebook de esta unidad.  Podéis utilizar el código presente en el Notebook, adaptándolo para resolver el ejercicio. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Respuesta**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejercicio 2\n",
    "\n",
    "Implementad un conjunto de funciones para obtener la **secuencia de ADN** del organismo *Homo sapiens* del cromosoma 1 (**chr1**) desde la posición 100000 hasta 101000 para la referencia **hg19**.  Para realizar el ejercicio utilizad la API de [UCSC](https://genome.ucsc.edu/goldenPath/help/api.html). **(1.5 puntos)** <span style=\"font-family: Courier New; background-color: #82b74b; color: #000000; padding: 2px; \">NM</span>\n",
    "\n",
    "**Nota**: El genoma de referencia de una célula es un repositorio de secuencias de ADN ( ácido desoxirribonucleico) empaquetado en forma de cromosoma. El ADN es un ácido nucleico que contiene la información genética que dirige el desarrollo y el funcionamiento de todos los seres vivos. El ADN se puede entender como una secuencia de nucleótidos (A, C, T y G) de una determinada longitud.  Este material hereditario codifica los genes que, una vez descifrados, son indispensables para la síntesis de las proteínas. \n",
    "\n",
    "Un genoma de referencia es la representación de la secuencia de ADN del genoma de una especie.  En el caso del organismo *Homo sapiens*, existen diferentes versiones del genoma de referencia.  La última versión, hg38, se publicó en el 2014 y es la más detallada y precisa.\n",
    "\n",
    "UCSC es un navegador de la Universidad de Santa Cruz de California que ofrece acceso a secuencias genómicas y su correspondiente anotación (genes, mRNAs, CpG,…) de una gran variedad de organismos, vertebrados e invertebrados. \n",
    "\n",
    "Referencia: [Genómica Computacional](http://discovery.uoc.edu/iii/encore/record/C__Rb1046448__Sgenomica%20Computacional__Orightresult__U__X7?lang=cat&suite=def). Enrique Blanco. Barcelona, Universitat Oberta de Catalunya, 2011.\n",
    "\n",
    "**Importante**: No es necesario entender toda la información que podéis obtener a través de la API de UCSC. Fijaros bien con lo que os pide el enunciado ( prestad atención a la palabras clave en negrita), y revisad los ejemplos de acceso a los datos que hay en la web de [UCSC](https://genome.ucsc.edu/goldenPath/help/api.html).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Respuesta**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import tweepy\n",
    "from IPython.display import HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def secuencia_adn(chromosome, start, end, version=\"hg19\"):\n",
    "    URL='https://api.genome.ucsc.edu/getData/sequence?genome={};track=knownGene;chrom=chr{};start={};end={}'.format(version, chromosome, start, end)\n",
    "    req = requests.get(URL)\n",
    "    data_adn = req.text\n",
    "    sec_adn = json.loads(data_adn)[\"dna\"]\n",
    "    return sec_adn\n",
    "\n",
    "def long_crom(chromosome, version=\"hg19\"):\n",
    "    url='https://api.genome.ucsc.edu/getData/sequence?genome={};track=knownGene;chrom=chr{};'.format(version, chromosome)\n",
    "    resp = requests.get(url=url)\n",
    "    data_adn = resp.text\n",
    "    sec_adn = json.loads(data_adn)[\"dna\"]\n",
    "    return len(sec_adn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'actaagcacacagagaataatgtctagaatctgagtgccatgttatcaaattgtactgagactcttgcagtcacacaggctgacatgtaagcatcgccatgcctagtacagactctccctgcagatgaaattatatgggatgctaaattataatgagaacaatgtttggtgagccaaaactacaacaagggaagctaattggatgaatttataaaaatatgcctcagccaaaatagcttaattcactctcccttatcataaggataatcttgcctaaagggacagtaatattAAAGACACTAGGAATAACCTCTGTACTTTGGACAGTAGACCTGCATAGCCCattaggcctcaatgaagtcttatgcaagaccagaagccaatttgccatttaaggtgattctccatgtttctgctctaaCTGTGCTTCACAATACTCAAAACACTAAATCAGGATGTTTCCTGGAGTTCAGGGAGCTGTCCGTGTTACTGAGCAGTTCTCAGCAACACAAAGATCCTACTGACTCCTCATCAGACTTCTTTCTCACTGGAATTTTACACCTGGGCTGTTAACACCAGGCCAGGTCAAATTCAAAGGAGAGAAAAAAGCTCATTATGAAGGGTAAAATCCAAAACACTGTGCATAAAGATATGGCACAATTTTTATACATAAAGATTTCATAAAACCAAAGCATCAGGAAATGAAAAGAGATACAGAAAGAAAAATGATGGTAAATGAGACATTAATTTACCCTTCTAATCTCTATCACAGCAAAAAGATAATTAAAAAATCTATATGAGGACCACAAAATACACAAAAATTATGTAGCAAAGCCTATAGCCTGAAAAAGTAAACATTGAAATTTGTATGTCCATAAAATGTTTACAAAATTCAGTACATATTACACACCCCACCCTAAAAACATCTAAGCAAAGTAGAGAATGTAGAAATGCTACAGATTATATTCTCTGATTATGACACAACAAAACTAGAAATTAC'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chromosome=1\n",
    "start=100000\n",
    "end=101000\n",
    "secuencia_adn(chromosome, start, end)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejercicio 3\n",
    "\n",
    "Dada la API de UCSC del ejercicio anterior, obtened la longitud del chr1 del organismo *Homo sapiens* según la versión del genoma de referencia hg19.  Calculad la diferencia entre la longitud del cromosoma chr1 entre las versiones hg19 y hg18. **(1.5 puntos)** <span style=\"font-family: Courier New; background-color: #82b74b; color: #000000; padding: 3px; \">NM</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Respuesta**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'long_crom' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-741bda36db97>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mchromosome\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0msec_hg19\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlong_crom\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchromosome\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mversion\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"hg19\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0msec_hg38\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlong_crom\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchromosome\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mversion\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"hg38\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mlen_difference\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msec_hg19\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0msec_hg38\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mlen_difference\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'long_crom' is not defined"
     ]
    }
   ],
   "source": [
    "chromosome=1\n",
    "sec_hg19 = long_crom(chromosome, version=\"hg19\")\n",
    "sec_hg38 = long_crom(chromosome, version=\"hg38\")\n",
    "len_difference = abs(sec_hg19-sec_hg38)\n",
    "len_difference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejercicio 4\n",
    "\n",
    "La [NASA](https://www.nasa.gov) mediante su [API](https://api.nasa.gov) publica cada día una imagen de astronomía.  Implementad una función para descargar y visualizar la imagen dentro del notebook. **(2 puntos)** <span style=\"font-family: Courier New; background-color: #82b74b; color: #000000; padding: 2px; \">NM</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Respuesta**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://apod.nasa.gov/apod/image/1912/ElectricMilkyWay_Pedretti_1080.jpg\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import urllib\n",
    "from IPython.display import Image\n",
    "from IPython.core.display import HTML \n",
    "\n",
    "URL = \"https://api.nasa.gov/planetary/apod?api_key=DEMO_KEY\"\n",
    "resp = requests.get(URL)\n",
    "imagen = json.loads(resp.text)[\"url\"]\n",
    "urllib.request.urlretrieve(imagen, \"photo.jpg\") #DOWNLOADS IMAGE\n",
    "Image(url=imagen) #Mostrar la imagen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejercicio 5\n",
    "\n",
    "[Scimago Journal](https://www.scimagojr.com/journalrank.php) es una web para consultar la información de las principales revistas de la comunidad científica. Programad un crawler que devuelva una tupla con el código y la área de todas las revistas que se muestran en la web.  Utilizad la estructura de crawler que hemos visto en el Notebook de esta unidad modificando únicamente dos líneas de código:\n",
    "\n",
    "- URL de inicio.\n",
    "- La expresión XPath que selecciona el contenido a capturar.\n",
    "\n",
    "**Nota**: si la ejecución del _crawler_ os devuelve un error `ReactorNotRestartable`, reiniciad el núcleo del Notebook (en el menú: `Kernel` - `Restart`). **(2 puntos)** <span style=\"font-family: Courier New; background-color: #ffcc5c; color: #000000; padding: 2px; \">EG</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Respuesta**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapy\n",
    "from scrapy.crawler import CrawlerProcess\n",
    "\n",
    "\n",
    "class uoc_spider(scrapy.Spider):\n",
    "    # asignamos un nombre a la araña\n",
    "    name = \"uoc_spider\"\n",
    "    # Indicamos la URL que queremos analizar.\n",
    "    # Incluimos aquí la URL de inicio:\n",
    "    7\n",
    "    ################################################\n",
    "    start_urls = [\n",
    "    \"https://www.scimagojr.com/journalrank.php\"\n",
    "    ]\n",
    "    ################################################\n",
    "\n",
    "    # Definimos el analizador.\n",
    "    def parse(self, response):\n",
    "        # Extraer el nombre de la moneda\n",
    "        # Incluir la expresión 'xpath' que nos devuelve los nombres de las␣\n",
    "        ################################################\n",
    "        for journal in response.xpath('//td[@class=\"tit\"]/a/text()'):\n",
    "        ################################################\n",
    "            yield {\n",
    "            'journal': journal.extract()\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creamos un crawler.\n",
    "process = CrawlerProcess({\n",
    "'USER_AGENT': 'Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1)',\n",
    "'DOWNLOAD_HANDLERS': {'s3': None},\n",
    "'LOG_ENABLED': True\n",
    "})\n",
    "# Inicializamos el crawler con la muestra de la araña.\n",
    "process.crawl(uoc_spider)\n",
    "# Lanzamos la araña.\n",
    "process.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejercicio 6\n",
    "\n",
    "Queremos conocer la Agenda de actos de  la Anella Olímpica de la ciudad de Barcelona.  Imprimid por pantalla el nombre del grupo o cantante que celebrará un concierto en la Anella Olímpica durante el año 2020.  Para realizar el ejercicio, consultad el portal de datos abiertos del Ayuntamiento de Barcelona mediante la siguiente [url](https://opendata-ajuntament.barcelona.cat/es/).  Primero tenéis que identificar qué métode utilizar para descargar los datos. Seguidamente, descargad los datos y procesarlos para responder la pregunta. \n",
    "**(2 puntos)** <span style=\"font-family: Courier New; background-color: #ffcc5c; color: #000000; padding: 2px; \">EG</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Respuesta**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'AbroadFest',\n",
       " 'Adexe & Nau',\n",
       " 'Arnau Griso',\n",
       " 'Beret',\n",
       " 'Bon Iver',\n",
       " 'Camila Cabello',\n",
       " 'Carlos Sadness',\n",
       " 'Dani Martín',\n",
       " 'Disney On Ice',\n",
       " 'Dream Theater',\n",
       " 'Dua Lipa',\n",
       " 'El Barrio',\n",
       " 'Elton John 2/10/2020',\n",
       " 'Elton John 3/10/2020',\n",
       " 'Fangoria',\n",
       " 'FernandoCosta',\n",
       " 'Halsey',\n",
       " 'Hatsune Miku',\n",
       " 'Hatsune Miku VIP',\n",
       " 'Iron Maiden',\n",
       " 'Izal',\n",
       " 'Jonas Brothers',\n",
       " 'Maldita Nerea',\n",
       " 'Maluma',\n",
       " 'Nick Cave',\n",
       " 'Nil Moliner',\n",
       " 'Paul McCartney',\n",
       " \"Ru Paul's Drag Race\",\n",
       " 'Sabaton',\n",
       " 'Tash Sultana',\n",
       " 'The 1975',\n",
       " 'The Cat Empire',\n",
       " 'Trial Indoor'}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "URL=\"https://api.bsmsa.eu/ext/api/ao/waos/ca/actosanella.json\"\n",
    "resp = requests.get(URL)\n",
    "data = json.loads(resp.text)\n",
    "\n",
    "bands=[]\n",
    "for info in data[\"actes\"][\"acte\"]:\n",
    "    if \"2020\" in info[\"date\"]:\n",
    "        bands.append(info[\"name\"])\n",
    "\n",
    "set(bands)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ejercicio Opcional"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modificad la función del ejercicio 5 para obtener las 10 primeras revistas asociadas en la área Computer Science del 2017.<span style=\"font-family: Courier New; background-color: #f2ae72; color: #000000; padding: 3px; \">EI</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Respuesta**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Soluciones ejercicios para practicar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejercicio 1\n",
    "\n",
    "Queremos saber los crímenes que se han producido en Reino Unido en una localización (latitud, longitud) y fecha concretas. Identificad qué métodos de la API siguiente podemos utilizar para obtener la información y contestad a las siguientes preguntas.<span style=\"font-family: Courier New; background-color: #82b74b; color: #000000; padding: 3px; \">NM</span> \n",
    "\n",
    "1. ¿A qué URL haremos la petición?\n",
    "2. ¿Qué tipo de petición HTTP (qué acción) deberemos realizar contra la API para obtener los datos deseados?\n",
    "3. ¿En qué formato obtendremos la respuesta de la API?\n",
    "4. ¿Qué parámetros deberemos proporcionar en la petición a la API?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Respuesta**\n",
    "1. https://data.police.uk/docs/method/crimes-at-location/\n",
    "2. Tenemos que realizar una petición tipo GET\n",
    "3. La respuesta la obtendremos en formato JSON\n",
    "4. Tenemos que proporcionar la fecha (date), latitud (lat) y longitud (lng)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejercicio 2 \n",
    "Programad una función que retorne el estado meteorológico actual en una cierta localización, definida por su código postal (**zip code**) y código de país (e.g: us, uk, es, fr, etc). La función debe devolver una lista de tuplas de dos elementos, correspondientes al resumen del estado actual del tiempo **(weather.main)** y a la descripción extendida **(weather.description)**. Utilizad la API de [openweathermap](https://openweathermap.org/api) para obtener las predicciones.<span style=\"font-family: Courier New; background-color: #82b74b; color: #000000; padding: 3px; \">NM</span>\n",
    "\n",
    "Para utilizar la API necesitareis registraros y obtener una API key. Podéis registraros [aquí](https://home.openweathermap.org/users/sign_up) y obtener vuestra API key [aquí](https://home.openweathermap.org/api_keys) una vez registrados. Tened en cuenta que la API key puede tardar un rato en funcionar después de registraros, y la API os devolverá un error 401 conforme la clave no es valida:\n",
    "\n",
    "`{\"cod\":401, \"message\": \"Invalid API key. Please see http://openweathermap.org/faq#error401 for more info.\"}`\n",
    "\n",
    "Simplemente esperad un rato antes de utilizar la clave.\n",
    "\n",
    "**Hints**: \n",
    "\n",
    "- Veréis que en general la API esta documentada sin incluir la API key, aun que esta es necesaria. Deberéis incluir la API key en la llamada como uno de los parámetros de la URL (&appid=your_api_key):\n",
    "\n",
    "    http://example_url.com?param1=value1&param2=value2&appid=your_api_key\n",
    "\n",
    "- Os animamos a que paséis por el proceso de registro para que veáis de que trata y cómo se generan las API keys. Aún así, os proporcionamos una API key en caso de que tengáis problemas con el proceso.\n",
    "\n",
    "    owm_api_key = 'd54f26dbcf6d4136bc0ef8ba5f07825b'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Respuesta**\n",
    "\n",
    "Lo primero que haremos será revisar la API de openweathermap para identificar qué endpoints nos pueden ser útiles.  El enunciado nos pide devolver el estado meteorológico actual dado un código postal, podemos utilizar https://openweathermap.org/current.\n",
    "\n",
    "Existe un método que nos devuelve el estado meteorológico a partir del código postal y el código del país separado por coma:\n",
    "\n",
    "api.openweathermap.org/data/2.5/weather?zip=zip_code,country_code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Clouds', 'scattered clouds')]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import requests\n",
    "def parse_response(response):\n",
    "    data = None\n",
    "    if response.status_code == 200:\n",
    "        # Data is formatted as JSON but received as string. Load it as JSON object\n",
    "        data = json.loads(response.content)        \n",
    "    \n",
    "    # Raise an error otherwise    \n",
    "    else:\n",
    "        raise Exception(\"Unexpected response (%s: %s).\" %(response.status_code, response.reason))\n",
    "    \n",
    "    return data\n",
    "\n",
    "\n",
    "def get_weather_zip(zip_code, country, api_key):\n",
    "    # Query the data from the API\n",
    "    base_url = 'http://api.openweathermap.org/data/2.5/weather?zip=%s,%s&appid=%s'\n",
    "    \n",
    "    # We also add the API KEY to the request\n",
    "    response = requests.get(base_url % (zip_code, country, api_key))\n",
    "    \n",
    "    # Check the response code and act accordingly\n",
    "    data = parse_response(response)\n",
    "    \n",
    "    # If the data was properly processed\n",
    "    if data:\n",
    "        weather = data.get('weather')\n",
    "        r = [(w.get('main'), w.get('description')) for w in weather]\n",
    "    else:\n",
    "        raise Exception(\"Couldn't get weather data.\")\n",
    "    \n",
    "    return r\n",
    "        \n",
    "api_key = '169af185292dd6119b14bc20d23400fb'\n",
    "zip_code = '08018'\n",
    "country_code = 'es'\n",
    "weather_data = get_weather_zip(zip_code, country_code, api_key)\n",
    "\n",
    "print (weather_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejercicio 3\n",
    "\n",
    "[CoinMarketCap](https://coinmarketcap.com/) es una web con contenido acerca de\n",
    "las 100 criptomonedas con más capitalización de mercado.Programad un _crawler_ que extraiga los nombres y la capitalización de todas les monedas que se muestran en CoinMarketCap. Para hacerlo, utilizad la estructura de _crawler_ que hemos visto en el Notebook de esta unidad **modificando únicamente dos lineas de código**:\n",
    "\n",
    "- URL de inicio.\n",
    "- La expresión XPath que selecciona el contenido a capturar.\n",
    "\n",
    "**Pista**: tal vez os puede ser de utilidad investigar sobre la scrapy shell y utilizarla para encontrar la expresión XPath que necesitas para resolver el ejercicio.\n",
    "\n",
    "**Nota**: si la ejecución del _crawler_ os devuelve un error `ReactorNotRestartable`, reiniciad el núcleo del Notebook (en el menú: `Kernel` - `Restart`).<span style=\"font-family: Courier New; background-color: #82b74b; color: #000000; padding: 3px; \">NM</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Respuesta**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-11-16 13:49:08 [scrapy.utils.log] INFO: Scrapy 1.7.4 started (bot: scrapybot)\n",
      "2019-11-16 13:49:08 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.9, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.7.3 (default, Mar 27 2019, 09:23:15) - [Clang 10.0.1 (clang-1001.0.46.3)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1d  10 Sep 2019), cryptography 2.8, Platform Darwin-19.0.0-x86_64-i386-64bit\n",
      "2019-11-16 13:49:08 [scrapy.crawler] INFO: Overridden settings: {'USER_AGENT': 'Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1)'}\n",
      "2019-11-16 13:49:08 [scrapy.extensions.telnet] INFO: Telnet Password: 3f21dc90b528ae78\n",
      "2019-11-16 13:49:08 [scrapy.middleware] INFO: Enabled extensions:\n",
      "['scrapy.extensions.corestats.CoreStats',\n",
      " 'scrapy.extensions.telnet.TelnetConsole',\n",
      " 'scrapy.extensions.memusage.MemoryUsage',\n",
      " 'scrapy.extensions.logstats.LogStats']\n",
      "2019-11-16 13:49:08 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
      "['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
      " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
      " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
      " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
      " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
      " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
      " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
      "2019-11-16 13:49:08 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
      "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
      " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
      " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
      " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
      " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
      "2019-11-16 13:49:08 [scrapy.middleware] INFO: Enabled item pipelines:\n",
      "[]\n",
      "2019-11-16 13:49:08 [scrapy.core.engine] INFO: Spider opened\n",
      "2019-11-16 13:49:08 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "2019-11-16 13:49:08 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n",
      "2019-11-16 13:49:08 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://coinmarketcap.com/> (referer: None)\n",
      "2019-11-16 13:49:09 [scrapy.core.scraper] DEBUG: Scraped from <200 https://coinmarketcap.com/>\n",
      "{'currency': 'Bitcoin'}\n",
      "2019-11-16 13:49:09 [scrapy.core.scraper] DEBUG: Scraped from <200 https://coinmarketcap.com/>\n",
      "{'currency': 'Ethereum'}\n",
      "2019-11-16 13:49:09 [scrapy.core.scraper] DEBUG: Scraped from <200 https://coinmarketcap.com/>\n",
      "{'currency': 'XRP'}\n",
      "2019-11-16 13:49:09 [scrapy.core.scraper] DEBUG: Scraped from <200 https://coinmarketcap.com/>\n",
      "{'currency': 'Bitcoin Cash'}\n",
      "2019-11-16 13:49:09 [scrapy.core.scraper] DEBUG: Scraped from <200 https://coinmarketcap.com/>\n",
      "{'currency': 'Tether'}\n",
      "2019-11-16 13:49:09 [scrapy.core.scraper] DEBUG: Scraped from <200 https://coinmarketcap.com/>\n",
      "{'currency': 'Litecoin'}\n",
      "2019-11-16 13:49:09 [scrapy.core.scraper] DEBUG: Scraped from <200 https://coinmarketcap.com/>\n",
      "{'currency': 'EOS'}\n",
      "2019-11-16 13:49:09 [scrapy.core.scraper] DEBUG: Scraped from <200 https://coinmarketcap.com/>\n",
      "{'currency': 'Binance Coin'}\n",
      "2019-11-16 13:49:09 [scrapy.core.scraper] DEBUG: Scraped from <200 https://coinmarketcap.com/>\n",
      "{'currency': 'Bitcoin SV'}\n",
      "2019-11-16 13:49:09 [scrapy.core.scraper] DEBUG: Scraped from <200 https://coinmarketcap.com/>\n",
      "{'currency': 'Stellar'}\n",
      "2019-11-16 13:49:09 [scrapy.core.scraper] DEBUG: Scraped from <200 https://coinmarketcap.com/>\n",
      "{'currency': 'TRON'}\n",
      "2019-11-16 13:49:09 [scrapy.core.scraper] DEBUG: Scraped from <200 https://coinmarketcap.com/>\n",
      "{'currency': 'Cardano'}\n",
      "2019-11-16 13:49:09 [scrapy.core.scraper] DEBUG: Scraped from <200 https://coinmarketcap.com/>\n",
      "{'currency': 'Monero'}\n",
      "2019-11-16 13:49:09 [scrapy.core.scraper] DEBUG: Scraped from <200 https://coinmarketcap.com/>\n",
      "{'currency': 'Chainlink'}\n",
      "2019-11-16 13:49:09 [scrapy.core.scraper] DEBUG: Scraped from <200 https://coinmarketcap.com/>\n",
      "{'currency': 'UNUS SED LEO'}\n",
      "2019-11-16 13:49:09 [scrapy.core.scraper] DEBUG: Scraped from <200 https://coinmarketcap.com/>\n",
      "{'currency': 'Huobi Token'}\n",
      "2019-11-16 13:49:09 [scrapy.core.scraper] DEBUG: Scraped from <200 https://coinmarketcap.com/>\n",
      "{'currency': 'NEO'}\n",
      "2019-11-16 13:49:09 [scrapy.core.scraper] DEBUG: Scraped from <200 https://coinmarketcap.com/>\n",
      "{'currency': 'Tezos'}\n",
      "2019-11-16 13:49:09 [scrapy.core.scraper] DEBUG: Scraped from <200 https://coinmarketcap.com/>\n",
      "{'currency': 'Cosmos'}\n",
      "2019-11-16 13:49:09 [scrapy.core.scraper] DEBUG: Scraped from <200 https://coinmarketcap.com/>\n",
      "{'currency': 'IOTA'}\n",
      "2019-11-16 13:49:09 [scrapy.core.scraper] DEBUG: Scraped from <200 https://coinmarketcap.com/>\n",
      "{'currency': 'Dash'}\n",
      "2019-11-16 13:49:09 [scrapy.core.scraper] DEBUG: Scraped from <200 https://coinmarketcap.com/>\n",
      "{'currency': 'Maker'}\n",
      "2019-11-16 13:49:09 [scrapy.core.scraper] DEBUG: Scraped from <200 https://coinmarketcap.com/>\n",
      "{'currency': 'Ethereum Classic'}\n",
      "2019-11-16 13:49:09 [scrapy.core.scraper] DEBUG: Scraped from <200 https://coinmarketcap.com/>\n",
      "{'currency': 'Ontology'}\n",
      "2019-11-16 13:49:09 [scrapy.core.scraper] DEBUG: Scraped from <200 https://coinmarketcap.com/>\n",
      "{'currency': 'USD Coin'}\n",
      "2019-11-16 13:49:09 [scrapy.core.scraper] DEBUG: Scraped from <200 https://coinmarketcap.com/>\n",
      "{'currency': 'Crypto.com Coin'}\n",
      "2019-11-16 13:49:09 [scrapy.core.scraper] DEBUG: Scraped from <200 https://coinmarketcap.com/>\n",
      "{'currency': 'VeChain'}\n",
      "2019-11-16 13:49:09 [scrapy.core.scraper] DEBUG: Scraped from <200 https://coinmarketcap.com/>\n",
      "{'currency': 'NEM'}\n",
      "2019-11-16 13:49:09 [scrapy.core.scraper] DEBUG: Scraped from <200 https://coinmarketcap.com/>\n",
      "{'currency': 'Basic Attention Token'}\n",
      "2019-11-16 13:49:09 [scrapy.core.scraper] DEBUG: Scraped from <200 https://coinmarketcap.com/>\n",
      "{'currency': 'Dogecoin'}\n",
      "2019-11-16 13:49:09 [scrapy.core.scraper] DEBUG: Scraped from <200 https://coinmarketcap.com/>\n",
      "{'currency': 'Zcash'}\n",
      "2019-11-16 13:49:09 [scrapy.core.scraper] DEBUG: Scraped from <200 https://coinmarketcap.com/>\n",
      "{'currency': 'Decred'}\n",
      "2019-11-16 13:49:09 [scrapy.core.scraper] DEBUG: Scraped from <200 https://coinmarketcap.com/>\n",
      "{'currency': 'Paxos Standard'}\n",
      "2019-11-16 13:49:09 [scrapy.core.scraper] DEBUG: Scraped from <200 https://coinmarketcap.com/>\n",
      "{'currency': 'Qtum'}\n",
      "2019-11-16 13:49:09 [scrapy.core.scraper] DEBUG: Scraped from <200 https://coinmarketcap.com/>\n",
      "{'currency': 'HedgeTrade'}\n",
      "2019-11-16 13:49:09 [scrapy.core.scraper] DEBUG: Scraped from <200 https://coinmarketcap.com/>\n",
      "{'currency': 'TrueUSD'}\n",
      "2019-11-16 13:49:09 [scrapy.core.scraper] DEBUG: Scraped from <200 https://coinmarketcap.com/>\n",
      "{'currency': '0x'}\n",
      "2019-11-16 13:49:09 [scrapy.core.scraper] DEBUG: Scraped from <200 https://coinmarketcap.com/>\n",
      "{'currency': 'Centrality'}\n",
      "2019-11-16 13:49:09 [scrapy.core.scraper] DEBUG: Scraped from <200 https://coinmarketcap.com/>\n",
      "{'currency': 'Holo'}\n",
      "2019-11-16 13:49:09 [scrapy.core.scraper] DEBUG: Scraped from <200 https://coinmarketcap.com/>\n",
      "{'currency': 'OmiseGO'}\n",
      "2019-11-16 13:49:09 [scrapy.core.scraper] DEBUG: Scraped from <200 https://coinmarketcap.com/>\n",
      "{'currency': 'Bitcoin Gold'}\n",
      "2019-11-16 13:49:09 [scrapy.core.scraper] DEBUG: Scraped from <200 https://coinmarketcap.com/>\n",
      "{'currency': 'V Systems'}\n",
      "2019-11-16 13:49:09 [scrapy.core.scraper] DEBUG: Scraped from <200 https://coinmarketcap.com/>\n",
      "{'currency': 'Ravencoin'}\n",
      "2019-11-16 13:49:09 [scrapy.core.scraper] DEBUG: Scraped from <200 https://coinmarketcap.com/>\n",
      "{'currency': 'Synthetix Network Token'}\n",
      "2019-11-16 13:49:09 [scrapy.core.scraper] DEBUG: Scraped from <200 https://coinmarketcap.com/>\n",
      "{'currency': 'ZB'}\n",
      "2019-11-16 13:49:09 [scrapy.core.scraper] DEBUG: Scraped from <200 https://coinmarketcap.com/>\n",
      "{'currency': 'LUNA'}\n",
      "2019-11-16 13:49:09 [scrapy.core.scraper] DEBUG: Scraped from <200 https://coinmarketcap.com/>\n",
      "{'currency': 'Nano'}\n",
      "2019-11-16 13:49:09 [scrapy.core.scraper] DEBUG: Scraped from <200 https://coinmarketcap.com/>\n",
      "{'currency': 'Augur'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-11-16 13:49:09 [scrapy.core.scraper] DEBUG: Scraped from <200 https://coinmarketcap.com/>\n",
      "{'currency': 'ABBC Coin'}\n",
      "2019-11-16 13:49:09 [scrapy.core.scraper] DEBUG: Scraped from <200 https://coinmarketcap.com/>\n",
      "{'currency': 'Algorand'}\n",
      "2019-11-16 13:49:09 [scrapy.core.scraper] DEBUG: Scraped from <200 https://coinmarketcap.com/>\n",
      "{'currency': 'Komodo'}\n",
      "2019-11-16 13:49:09 [scrapy.core.scraper] DEBUG: Scraped from <200 https://coinmarketcap.com/>\n",
      "{'currency': 'Bytom'}\n",
      "2019-11-16 13:49:09 [scrapy.core.scraper] DEBUG: Scraped from <200 https://coinmarketcap.com/>\n",
      "{'currency': 'Dai'}\n",
      "2019-11-16 13:49:09 [scrapy.core.scraper] DEBUG: Scraped from <200 https://coinmarketcap.com/>\n",
      "{'currency': 'EDUCare'}\n",
      "2019-11-16 13:49:09 [scrapy.core.scraper] DEBUG: Scraped from <200 https://coinmarketcap.com/>\n",
      "{'currency': 'KuCoin Shares'}\n",
      "2019-11-16 13:49:09 [scrapy.core.scraper] DEBUG: Scraped from <200 https://coinmarketcap.com/>\n",
      "{'currency': 'Lisk'}\n",
      "2019-11-16 13:49:09 [scrapy.core.scraper] DEBUG: Scraped from <200 https://coinmarketcap.com/>\n",
      "{'currency': 'Silverway'}\n",
      "2019-11-16 13:49:09 [scrapy.core.scraper] DEBUG: Scraped from <200 https://coinmarketcap.com/>\n",
      "{'currency': 'Bitcoin Diamond'}\n",
      "2019-11-16 13:49:09 [scrapy.core.scraper] DEBUG: Scraped from <200 https://coinmarketcap.com/>\n",
      "{'currency': 'BitTorrent'}\n",
      "2019-11-16 13:49:09 [scrapy.core.scraper] DEBUG: Scraped from <200 https://coinmarketcap.com/>\n",
      "{'currency': 'DigiByte'}\n",
      "2019-11-16 13:49:09 [scrapy.core.scraper] DEBUG: Scraped from <200 https://coinmarketcap.com/>\n",
      "{'currency': 'Siacoin'}\n",
      "2019-11-16 13:49:09 [scrapy.core.scraper] DEBUG: Scraped from <200 https://coinmarketcap.com/>\n",
      "{'currency': 'Seele'}\n",
      "2019-11-16 13:49:09 [scrapy.core.scraper] DEBUG: Scraped from <200 https://coinmarketcap.com/>\n",
      "{'currency': 'Quant'}\n",
      "2019-11-16 13:49:09 [scrapy.core.scraper] DEBUG: Scraped from <200 https://coinmarketcap.com/>\n",
      "{'currency': 'ICON'}\n",
      "2019-11-16 13:49:09 [scrapy.core.scraper] DEBUG: Scraped from <200 https://coinmarketcap.com/>\n",
      "{'currency': 'THETA'}\n",
      "2019-11-16 13:49:09 [scrapy.core.scraper] DEBUG: Scraped from <200 https://coinmarketcap.com/>\n",
      "{'currency': 'FTX Token'}\n",
      "2019-11-16 13:49:09 [scrapy.core.scraper] DEBUG: Scraped from <200 https://coinmarketcap.com/>\n",
      "{'currency': 'IOST'}\n",
      "2019-11-16 13:49:09 [scrapy.core.scraper] DEBUG: Scraped from <200 https://coinmarketcap.com/>\n",
      "{'currency': 'Swipe'}\n",
      "2019-11-16 13:49:09 [scrapy.core.scraper] DEBUG: Scraped from <200 https://coinmarketcap.com/>\n",
      "{'currency': 'Verge'}\n",
      "2019-11-16 13:49:09 [scrapy.core.scraper] DEBUG: Scraped from <200 https://coinmarketcap.com/>\n",
      "{'currency': 'Waves'}\n",
      "2019-11-16 13:49:09 [scrapy.core.scraper] DEBUG: Scraped from <200 https://coinmarketcap.com/>\n",
      "{'currency': 'HyperCash'}\n",
      "2019-11-16 13:49:09 [scrapy.core.scraper] DEBUG: Scraped from <200 https://coinmarketcap.com/>\n",
      "{'currency': 'Bytecoin'}\n",
      "2019-11-16 13:49:09 [scrapy.core.scraper] DEBUG: Scraped from <200 https://coinmarketcap.com/>\n",
      "{'currency': 'DxChain Token'}\n",
      "2019-11-16 13:49:09 [scrapy.core.scraper] DEBUG: Scraped from <200 https://coinmarketcap.com/>\n",
      "{'currency': 'BitShares'}\n",
      "2019-11-16 13:49:09 [scrapy.core.scraper] DEBUG: Scraped from <200 https://coinmarketcap.com/>\n",
      "{'currency': 'MonaCoin'}\n",
      "2019-11-16 13:49:09 [scrapy.core.scraper] DEBUG: Scraped from <200 https://coinmarketcap.com/>\n",
      "{'currency': 'MCO'}\n",
      "2019-11-16 13:49:09 [scrapy.core.scraper] DEBUG: Scraped from <200 https://coinmarketcap.com/>\n",
      "{'currency': 'Aeternity'}\n",
      "2019-11-16 13:49:09 [scrapy.core.scraper] DEBUG: Scraped from <200 https://coinmarketcap.com/>\n",
      "{'currency': 'Nexo'}\n",
      "2019-11-16 13:49:09 [scrapy.core.scraper] DEBUG: Scraped from <200 https://coinmarketcap.com/>\n",
      "{'currency': 'MaidSafeCoin'}\n",
      "2019-11-16 13:49:09 [scrapy.core.scraper] DEBUG: Scraped from <200 https://coinmarketcap.com/>\n",
      "{'currency': 'iExec RLC'}\n",
      "2019-11-16 13:49:09 [scrapy.core.scraper] DEBUG: Scraped from <200 https://coinmarketcap.com/>\n",
      "{'currency': 'Zilliqa'}\n",
      "2019-11-16 13:49:09 [scrapy.core.scraper] DEBUG: Scraped from <200 https://coinmarketcap.com/>\n",
      "{'currency': 'BitMax Token'}\n",
      "2019-11-16 13:49:09 [scrapy.core.scraper] DEBUG: Scraped from <200 https://coinmarketcap.com/>\n",
      "{'currency': 'Ardor'}\n",
      "2019-11-16 13:49:09 [scrapy.core.scraper] DEBUG: Scraped from <200 https://coinmarketcap.com/>\n",
      "{'currency': 'Steem'}\n",
      "2019-11-16 13:49:09 [scrapy.core.scraper] DEBUG: Scraped from <200 https://coinmarketcap.com/>\n",
      "{'currency': 'Chiliz'}\n",
      "2019-11-16 13:49:09 [scrapy.core.scraper] DEBUG: Scraped from <200 https://coinmarketcap.com/>\n",
      "{'currency': 'Aurora'}\n",
      "2019-11-16 13:49:09 [scrapy.core.scraper] DEBUG: Scraped from <200 https://coinmarketcap.com/>\n",
      "{'currency': 'Enjin Coin'}\n",
      "2019-11-16 13:49:09 [scrapy.core.scraper] DEBUG: Scraped from <200 https://coinmarketcap.com/>\n",
      "{'currency': 'Energi'}\n",
      "2019-11-16 13:49:09 [scrapy.core.scraper] DEBUG: Scraped from <200 https://coinmarketcap.com/>\n",
      "{'currency': 'aelf'}\n",
      "2019-11-16 13:49:09 [scrapy.core.scraper] DEBUG: Scraped from <200 https://coinmarketcap.com/>\n",
      "{'currency': 'RIF Token'}\n",
      "2019-11-16 13:49:09 [scrapy.core.scraper] DEBUG: Scraped from <200 https://coinmarketcap.com/>\n",
      "{'currency': 'Horizen'}\n",
      "2019-11-16 13:49:09 [scrapy.core.scraper] DEBUG: Scraped from <200 https://coinmarketcap.com/>\n",
      "{'currency': 'Ren'}\n",
      "2019-11-16 13:49:09 [scrapy.core.scraper] DEBUG: Scraped from <200 https://coinmarketcap.com/>\n",
      "{'currency': 'Golem'}\n",
      "2019-11-16 13:49:09 [scrapy.core.scraper] DEBUG: Scraped from <200 https://coinmarketcap.com/>\n",
      "{'currency': 'Status'}\n",
      "2019-11-16 13:49:09 [scrapy.core.scraper] DEBUG: Scraped from <200 https://coinmarketcap.com/>\n",
      "{'currency': 'UNI COIN'}\n",
      "2019-11-16 13:49:09 [scrapy.core.scraper] DEBUG: Scraped from <200 https://coinmarketcap.com/>\n",
      "{'currency': 'Newton'}\n",
      "2019-11-16 13:49:09 [scrapy.core.scraper] DEBUG: Scraped from <200 https://coinmarketcap.com/>\n",
      "{'currency': 'ILCoin'}\n",
      "2019-11-16 13:49:09 [scrapy.core.scraper] DEBUG: Scraped from <200 https://coinmarketcap.com/>\n",
      "{'currency': 'Crypterium'}\n",
      "2019-11-16 13:49:09 [scrapy.core.scraper] DEBUG: Scraped from <200 https://coinmarketcap.com/>\n",
      "{'currency': 'Pundi X'}\n",
      "2019-11-16 13:49:09 [scrapy.core.scraper] DEBUG: Scraped from <200 https://coinmarketcap.com/>\n",
      "{'currency': 'GXChain'}\n",
      "2019-11-16 13:49:09 [scrapy.core.engine] INFO: Closing spider (finished)\n",
      "2019-11-16 13:49:09 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
      "{'downloader/request_bytes': 232,\n",
      " 'downloader/request_count': 1,\n",
      " 'downloader/request_method_count/GET': 1,\n",
      " 'downloader/response_bytes': 54284,\n",
      " 'downloader/response_count': 1,\n",
      " 'downloader/response_status_count/200': 1,\n",
      " 'elapsed_time_seconds': 0.543706,\n",
      " 'finish_reason': 'finished',\n",
      " 'finish_time': datetime.datetime(2019, 11, 16, 12, 49, 9, 341609),\n",
      " 'item_scraped_count': 100,\n",
      " 'log_count/DEBUG': 101,\n",
      " 'log_count/INFO': 10,\n",
      " 'memusage/max': 66461696,\n",
      " 'memusage/startup': 66461696,\n",
      " 'response_received_count': 1,\n",
      " 'scheduler/dequeued': 1,\n",
      " 'scheduler/dequeued/memory': 1,\n",
      " 'scheduler/enqueued': 1,\n",
      " 'scheduler/enqueued/memory': 1,\n",
      " 'start_time': datetime.datetime(2019, 11, 16, 12, 49, 8, 797903)}\n",
      "2019-11-16 13:49:09 [scrapy.core.engine] INFO: Spider closed (finished)\n"
     ]
    }
   ],
   "source": [
    "import scrapy\n",
    "from scrapy.crawler import CrawlerProcess\n",
    "\n",
    "# Creamos la araña.\n",
    "class uoc_spider(scrapy.Spider):\n",
    "    \n",
    "    # asignamos un nombre a la araña\n",
    "    name = \"uoc_spider\"\n",
    "    \n",
    "    # Indicamos la URL que queremos analizar.\n",
    "    # Incluimos aquí la URL de inicio:\n",
    "\n",
    "    ################################################\n",
    "    start_urls = [\n",
    "        \"https://coinmarketcap.com/\"\n",
    "    ]\n",
    "    ################################################\n",
    "    \n",
    "    # Definimos el analizador.\n",
    "    def parse(self, response):\n",
    "        # Extraer el nombre de la moneda\n",
    "        # Incluir la expresión 'xpath' que nos devuelve los nombres de las monedas.\n",
    "    ################################################\n",
    "        for currency in response.xpath('//td[@class=\"no-wrap currency-name\"]/@data-sort'):\n",
    "        ################################################\n",
    "            yield {\n",
    "                'currency': currency.extract()\n",
    "            }\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    # Creamos un crawler.\n",
    "    process = CrawlerProcess({\n",
    "        'USER_AGENT': 'Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1)',\n",
    "        'DOWNLOAD_HANDLERS': {'s3': None},\n",
    "        'LOG_ENABLED': True\n",
    "    })\n",
    "    \n",
    "    # Inicializamos el crawler con la muestra de la araña.\n",
    "    process.crawl(uoc_spider)\n",
    "    \n",
    "    # Lanzamos la aranya.\n",
    "    process.start()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
